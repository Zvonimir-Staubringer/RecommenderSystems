# filepath: z:\Zavrsni rad\RecommenderSystems\HybridRecommender MovieLens.py
import os
import random
import numpy as np
import pandas as pd
from collections import defaultdict
from matplotlib import pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from sklearn.metrics import mean_absolute_error, mean_squared_error, roc_curve, auc
from surprise import Dataset, Reader, SVD, accuracy

# --- Load shared train/test split (generated by data_split.py) ---
try:
    train_df = pd.read_csv('MovieLens/train_df.csv')
    test_df = pd.read_csv('MovieLens/test_df.csv')
    print("Loaded shared train_df/test_df")
except Exception:
    raise FileNotFoundError("MovieLens/train_df.csv and MovieLens/test_df.csv required. Run data_split.py first.")

# --- Load metadata & links and build consistent movieId <-> title mapping ---
metadata = pd.read_csv('MovieLens/movies_metadata.csv', low_memory=False)
links = pd.read_csv('MovieLens/links_small.csv')

# keep only numeric metadata ids
metadata = metadata[metadata['id'].apply(lambda x: str(x).isdigit())].copy()
metadata['id'] = metadata['id'].astype(int)

links = links[links['tmdbId'].notnull()].copy()
links['tmdbId'] = links['tmdbId'].astype(int)

# merge metadata (tmdb id) with links (MovieLens movieId) to get titles keyed by movieId
# safe-select only columns that actually exist in metadata (some datasets miss 'keywords' etc.)
cols_needed = ['id', 'title', 'overview', 'genres', 'tagline', 'original_language', 'keywords']
cols_present = [c for c in cols_needed if c in metadata.columns]
meta_part = metadata[cols_present].copy()

# ensure all needed cols exist (fill missing with empty strings) so later code can rely on them
for c in cols_needed:
    if c not in meta_part.columns:
        meta_part[c] = ''

meta_links = pd.merge(
    links[['movieId', 'tmdbId']],
    meta_part,
    left_on='tmdbId',
    right_on='id',
    how='inner'
)
meta_links = meta_links.rename(columns={'title': 'title_meta'})
meta_links = meta_links.drop_duplicates(subset=['movieId']).reset_index(drop=True)

movieid_to_title = dict(zip(meta_links['movieId'].astype(int), meta_links['title_meta']))
title_to_movieid = {v: k for k, v in movieid_to_title.items()}

# filter train/test to movies present in mapping
train_df = train_df[train_df['movieId'].isin(movieid_to_title.keys())].copy()
test_df = test_df[test_df['movieId'].isin(movieid_to_title.keys())].copy()

# build metadata_subset indexed by our merged meta_links order
metadata_subset = meta_links.reset_index(drop=True)
# prepare text content
for col in ['overview', 'tagline', 'genres', 'keywords', 'original_language']:
    if col not in metadata_subset.columns:
        metadata_subset[col] = ''
metadata_subset['genres_str'] = metadata_subset['genres'].fillna('').astype(str)
metadata_subset['keywords_str'] = metadata_subset['keywords'].fillna('').astype(str)
metadata_subset['content'] = (metadata_subset['overview'].fillna('') + ' ' +
                              metadata_subset['genres_str'] + ' ' +
                              metadata_subset['tagline'].fillna('') + ' ' +
                              metadata_subset['original_language'].fillna('') + ' ' +
                              metadata_subset['keywords_str'])

# --- TF-IDF / content similarity --- 
tfidf = TfidfVectorizer(stop_words='english', min_df=2, ngram_range=(1,1))
tfidf_matrix = tfidf.fit_transform(metadata_subset['content'])
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

# mapping movieId -> row index in metadata_subset
movieid_to_idx = dict(zip(metadata_subset['movieId'].astype(int), metadata_subset.index))
idx_to_movieid = dict(zip(metadata_subset.index, metadata_subset['movieId'].astype(int)))

# --- Surprise SVD trained on shared train_df ---
reader = Reader(rating_scale=(0.5, 5.0))
data_train = Dataset.load_from_df(train_df[['userId', 'movieId', 'rating']], reader)
trainset = data_train.build_full_trainset()
svd = SVD(n_factors=50, random_state=42)
svd.fit(trainset)

# prepare testset for Surprise evaluation (all test rows)
testset_surprise = list(zip(test_df['userId'].astype(int), test_df['movieId'].astype(int), test_df['rating'].astype(float)))
predictions_surprise = svd.test(testset_surprise)
mae_collab_all = accuracy.mae(predictions_surprise, verbose=False)
rmse_collab_all = accuracy.rmse(predictions_surprise, verbose=False)

# --- Candidate pool: limit to sufficiently popular movies in train to reduce noise ---
min_ratings = 5
pop_counts = train_df['movieId'].value_counts()
popular = pop_counts[pop_counts >= min_ratings].index.tolist()
min_pool = 1000
if len(popular) < min_pool:
    popular = pop_counts.index[:min_pool].tolist()
candidate_movie_ids = [mid for mid in popular if mid in movieid_to_title]

# helpers
def get_user_seen_train(uid):
    return set(train_df[train_df['userId'] == uid]['movieId'].unique())

def content_score_for_query(query_mid, candidates):
    q_idx = movieid_to_idx.get(int(query_mid))
    if q_idx is None:
        return {mid: 0.0 for mid in candidates}
    sims = np.asarray(cosine_sim[q_idx]).flatten()
    out = {}
    for mid in candidates:
        idx = movieid_to_idx.get(int(mid))
        out[mid] = float(sims[idx]) if idx is not None else 0.0
    return out

def content_predict_rating(uid, test_mid):
    # weighted average of user's train items by content similarity to test item
    train_rows = train_df[train_df['userId'] == uid]
    if train_rows.empty:
        return test_df['rating'].mean()  # fallback
    weights = 0.0
    weighted_sum = 0.0
    for _, r in train_rows.iterrows():
        tr_mid = int(r['movieId'])
        tr_rating = float(r['rating'])
        sim = 0.0
        if tr_mid in movieid_to_idx and test_mid in movieid_to_idx:
            sim = float(cosine_sim[movieid_to_idx[tr_mid], movieid_to_idx[test_mid]])
        if sim > 0:
            weights += sim
            weighted_sum += sim * tr_rating
    if weights > 0:
        return weighted_sum / weights
    return float(train_rows['rating'].mean())

# ranking functions return ordered movieIds
def rank_collab(uid, candidates, top_n):
    preds = []
    for mid in candidates:
        p = svd.predict(uid, mid)
        preds.append((mid, p.est))
    preds.sort(key=lambda x: x[1], reverse=True)
    return [mid for mid, _ in preds[:top_n]]

def rank_content(uid, query_mid, candidates, top_n):
    scores = content_score_for_query(query_mid, candidates)
    ranked = sorted(candidates, key=lambda m: scores.get(m, 0.0), reverse=True)
    return ranked[:top_n]

def rank_hybrid(uid, query_mid, candidates, top_n, alpha=0.5):
    # content in [0,1], collab est normalized to [0,1]
    content_scores = content_score_for_query(query_mid, candidates)
    hybrid_scores = {}
    for mid in candidates:
        collab_est = svd.predict(uid, mid).est
        collab_norm = (collab_est - 0.5) / 4.5
        cscore = content_scores.get(mid, 0.0)
        hybrid_scores[mid] = alpha * cscore + (1 - alpha) * collab_norm
    ranked = sorted(candidates, key=lambda m: hybrid_scores.get(m, 0.0), reverse=True)
    return ranked[:top_n]

# --- Evaluation: use same train/test split for all methods ---
def evaluate_all_methods(N=10, alpha=0.5):
    users = sorted(test_df['userId'].unique())
    precisions = {'content': [], 'collab': [], 'hybrid': []}
    aps = {'content': [], 'collab': [], 'hybrid': []}
    recalls = {'content': [], 'collab': [], 'hybrid': []}   # ADDED: collect recall per user
    # MAE/RMSE: compute predictions for all test rows per method
    true_vals = list(test_df['rating'].astype(float))
    # collab predictions already computed (predictions_surprise)
    pred_collab_map = {(int(p.uid), int(p.iid)): p.est for p in predictions_surprise}
    pred_content_all = []
    pred_hybrid_all = []

    users_evaluated = 0
    for uid in users:
        # build user's relevant set from test_df
        relevant = set(test_df[test_df['userId'] == uid]['movieId'].astype(int).tolist())
        if not relevant:
            continue
        seen = get_user_seen_train(uid)
        # choose query: user's highest-rated train item (fallback to any seen)
        train_rows = train_df[train_df['userId'] == uid]
        if train_rows.empty:
            continue
        best_mid = int(train_rows.loc[train_rows['rating'].idxmax()]['movieId'])
        # candidate pool excluding seen
        candidates = [mid for mid in candidate_movie_ids if mid not in seen]
        if not candidates:
            continue

        # get top-N per method
        top_content = rank_content(uid, best_mid, candidates, N)
        top_collab = rank_collab(uid, candidates, N)
        top_hybrid = rank_hybrid(uid, best_mid, candidates, N, alpha=alpha)

        for name, top in (('content', top_content), ('collab', top_collab), ('hybrid', top_hybrid)):
            # precision@N
            tp = len(set(top) & relevant)
            precisions[name].append(tp / N)
            # recall@N (odziv): tp / broj relevantnih (po korisniku)
            recalls[name].append(tp / len(relevant))
            # AP@N
            hits = 0
            sum_prec = 0.0
            for i, mid in enumerate(top):
                if mid in relevant:
                    hits += 1
                    sum_prec += hits / (i + 1)
            ap = sum_prec / min(len(relevant), N) if relevant else 0.0
            aps[name].append(ap)

        users_evaluated += 1

    # Nakon što smo izračunali ranking metrike, izračunaj predikcije za MAE/RMSE
    # tako da su u istom redoslijedu kao test_df (osigurava konzistentne duljine)
    pred_content_all = []
    pred_hybrid_all = []
    for _, trow in test_df.iterrows():
        uid = int(trow['userId'])
        test_mid = int(trow['movieId'])
        # content pred on test row (uses user's train rows inside)
        cont_pred = content_predict_rating(uid, test_mid)
        pred_content_all.append(float(cont_pred))
        # collab pred: try to use previously computed mapping, fallback to svd.predict
        collab_pred = pred_collab_map.get((uid, test_mid))
        if collab_pred is None:
            collab_pred = svd.predict(uid, test_mid).est
        hybrid_pred = alpha * cont_pred + (1 - alpha) * collab_pred
        pred_hybrid_all.append(float(hybrid_pred))

    # aggregate ranking metrics
    out = {}
    for name in ['content', 'collab', 'hybrid']:
        out[name] = {
            'users_evaluated': users_evaluated,
            'precision_at_10': float(np.mean(precisions[name])) if precisions[name] else float('nan'),
            'map10': float(np.mean(aps[name])) if aps[name] else float('nan'),
            'recall_at_10': float(np.mean(recalls[name])) if recalls[name] else float('nan')  # ADDED: mean recall
        }

    # MAE/RMSE: collab from Surprise, content/hybrid from assembled predictions
    out['collab']['mae'] = float(mean_absolute_error(true_vals, [pred_collab_map.get((int(uid), int(iid)), 3.0) for uid, iid in zip(test_df['userId'], test_df['movieId'])]))
    out['collab']['rmse'] = float(np.sqrt(mean_squared_error(true_vals, [pred_collab_map.get((int(uid), int(iid)), 3.0) for uid, iid in zip(test_df['userId'], test_df['movieId'])])))

    out['content']['mae'] = float(mean_absolute_error(true_vals, pred_content_all)) if pred_content_all else float('nan')
    out['content']['rmse'] = float(np.sqrt(mean_squared_error(true_vals, pred_content_all))) if pred_content_all else float('nan')

    out['hybrid']['mae'] = float(mean_absolute_error(true_vals, pred_hybrid_all)) if pred_hybrid_all else float('nan')
    out['hybrid']['rmse'] = float(np.sqrt(mean_squared_error(true_vals, pred_hybrid_all))) if pred_hybrid_all else float('nan')

    return out

# Run evaluation and print results
res = evaluate_all_methods(N=10, alpha=0.5)
for method, vals in res.items():
    print(f"\nMethod: {method}")
    print(f"Evaluated users (approx): {vals['users_evaluated']}")
    print(f"Precision@10: {vals['precision_at_10']:.4f}")
    print(f"Recall@10: {vals['recall_at_10']:.4f}")   # ADDED
    print(f"MAP@10: {vals['map10']:.4f}")
    if not np.isnan(vals.get('mae', np.nan)):
        print(f"MAE: {vals['mae']:.4f}")
        print(f"RMSE: {vals['rmse']:.4f}")
    else:
        print("Nema dovoljno podataka za MAE/RMSE za ovu metodu.")

# Optional: ROC for collaborative predictions on testset
try:
    y_true = [1 if r >= 4.0 else 0 for r in test_df['rating'].astype(float)]
    y_score = [p.est for p in predictions_surprise]
    if y_true and y_score:
        fpr, tpr, _ = roc_curve(y_true, y_score)
        roc_auc = auc(fpr, tpr)
        plt.figure(figsize=(7,5))
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC={roc_auc:.2f})')
        plt.plot([0,1],[0,1], color='navy', lw=2, linestyle='--')
        plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
        plt.title('ROC (Collaborative SVD)')
        plt.legend(loc='lower right'); plt.tight_layout(); plt.show()
except Exception:
    pass